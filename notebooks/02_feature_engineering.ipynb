{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook transforms raw credit card transaction data into meaningful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering for Credit Card Fraud Detection\n",
      "==================================================\n",
      "Loading data...\n",
      "Data loaded successfully. Shape: (284807, 31)\n",
      "\n",
      "Dataset Info:\n",
      "Shape: (284807, 31)\n",
      "Fraud cases: 492 (0.17%)\n",
      "Normal cases: 284315 (99.83%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering for Credit Card Fraud Detection\")\n",
    "print(\"=\" * 50)\n",
    "# Load the raw data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/raw/creditcard.csv')\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"creditcard.csv not found. Using sample data structure...\")\n",
    "    # Create sample data structure for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    df = pd.DataFrame({\n",
    "        'Time': np.random.randint(0, 172800, n_samples),  # 48 hours in seconds\n",
    "        'V1': np.random.normal(0, 1, n_samples),\n",
    "        'V2': np.random.normal(0, 1, n_samples),\n",
    "        'V3': np.random.normal(0, 1, n_samples),\n",
    "        'V4': np.random.normal(0, 1, n_samples),\n",
    "        'V5': np.random.normal(0, 1, n_samples),\n",
    "        'V6': np.random.normal(0, 1, n_samples),\n",
    "        'V7': np.random.normal(0, 1, n_samples),\n",
    "        'V8': np.random.normal(0, 1, n_samples),\n",
    "        'V9': np.random.normal(0, 1, n_samples),\n",
    "        'V10': np.random.normal(0, 1, n_samples),\n",
    "        'V11': np.random.normal(0, 1, n_samples),\n",
    "        'V12': np.random.normal(0, 1, n_samples),\n",
    "        'V13': np.random.normal(0, 1, n_samples),\n",
    "        'V14': np.random.normal(0, 1, n_samples),\n",
    "        'V15': np.random.normal(0, 1, n_samples),\n",
    "        'V16': np.random.normal(0, 1, n_samples),\n",
    "        'V17': np.random.normal(0, 1, n_samples),\n",
    "        'V18': np.random.normal(0, 1, n_samples),\n",
    "        'V19': np.random.normal(0, 1, n_samples),\n",
    "        'V20': np.random.normal(0, 1, n_samples),\n",
    "        'V21': np.random.normal(0, 1, n_samples),\n",
    "        'V22': np.random.normal(0, 1, n_samples),\n",
    "        'V23': np.random.normal(0, 1, n_samples),\n",
    "        'V24': np.random.normal(0, 1, n_samples),\n",
    "        'V25': np.random.normal(0, 1, n_samples),\n",
    "        'V26': np.random.normal(0, 1, n_samples),\n",
    "        'V27': np.random.normal(0, 1, n_samples),\n",
    "        'V28': np.random.normal(0, 1, n_samples),\n",
    "        'Amount': np.random.lognormal(2, 1.5, n_samples),\n",
    "        'Class': np.random.choice([0, 1], n_samples, p=[0.998, 0.002])  # Imbalanced classes\n",
    "    })\n",
    "    print(\"Sample data created for demonstration\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Fraud cases: {df['Class'].sum()} ({df['Class'].mean()*100:.2f}%)\")\n",
    "print(f\"Normal cases: {(df['Class'] == 0).sum()} ({(df['Class'] == 0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating Temporal Features...\n",
      "------------------------------\n",
      "Temporal features created:\n",
      "- Hour, Day\n",
      "- Hour_sin, Hour_cos (cyclical encoding)\n",
      "- Is_Weekend, Is_Night, Is_Business_Hours\n"
     ]
    }
   ],
   "source": [
    "# 1. TEMPORAL FEATURES\n",
    "print(\"\\n1. Creating Temporal Features...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Convert time to hours and extract temporal patterns\n",
    "df['Hour'] = (df['Time'] / 3600) % 24\n",
    "df['Day'] = df['Time'] // (24 * 3600)\n",
    "\n",
    "# Create time-based features\n",
    "df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "\n",
    "# Time periods\n",
    "df['Is_Weekend'] = (df['Day'] % 7 >= 5).astype(int)\n",
    "df['Is_Night'] = ((df['Hour'] >= 22) | (df['Hour'] <= 6)).astype(int)\n",
    "df['Is_Business_Hours'] = ((df['Hour'] >= 9) & (df['Hour'] <= 17)).astype(int)\n",
    "\n",
    "print(\"Temporal features created:\")\n",
    "print(\"- Hour, Day\")\n",
    "print(\"- Hour_sin, Hour_cos (cyclical encoding)\")\n",
    "print(\"- Is_Weekend, Is_Night, Is_Business_Hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Creating Amount-based Features...\n",
      "------------------------------\n",
      "Amount-based features created:\n",
      "- Amount_log (log transformation)\n",
      "- Amount_Category (categorical bins)\n",
      "- Amount_Percentile (percentile bins)\n",
      "- Amount_Squared, Amount_Sqrt\n"
     ]
    }
   ],
   "source": [
    "# 2. AMOUNT-BASED FEATURES\n",
    "print(\"\\n2. Creating Amount-based Features...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Log transform for amount (handle skewness)\n",
    "df['Amount_log'] = np.log1p(df['Amount'])\n",
    "\n",
    "# Amount categories\n",
    "df['Amount_Category'] = pd.cut(df['Amount'], \n",
    "                              bins=[0, 50, 200, 1000, float('inf')], \n",
    "                              labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "\n",
    "# Amount percentiles\n",
    "df['Amount_Percentile'] = pd.qcut(df['Amount'], \n",
    "                                 q=10, \n",
    "                                 labels=False, \n",
    "                                 duplicates='drop')\n",
    "\n",
    "# Amount statistics\n",
    "df['Amount_Squared'] = df['Amount'] ** 2\n",
    "df['Amount_Sqrt'] = np.sqrt(df['Amount'])\n",
    "\n",
    "print(\"Amount-based features created:\")\n",
    "print(\"- Amount_log (log transformation)\")\n",
    "print(\"- Amount_Category (categorical bins)\")\n",
    "print(\"- Amount_Percentile (percentile bins)\")\n",
    "print(\"- Amount_Squared, Amount_Sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Creating Velocity Features...\n",
      "------------------------------\n",
      "Velocity features created:\n",
      "- Time_Delta (time between transactions)\n",
      "- Velocity (transaction frequency)\n",
      "- Rolling statistics for windows: 5, 10, 20\n"
     ]
    }
   ],
   "source": [
    "# 3. VELOCITY FEATURES (Transaction Frequency)\n",
    "print(\"\\n3. Creating Velocity Features...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Sort by time for velocity calculations\n",
    "df_sorted = df.sort_values('Time').reset_index(drop=True)\n",
    "\n",
    "# Calculate time differences between consecutive transactions\n",
    "df_sorted['Time_Delta'] = df_sorted['Time'].diff()\n",
    "df_sorted['Time_Delta'] = df_sorted['Time_Delta'].fillna(df_sorted['Time_Delta'].median())\n",
    "\n",
    "# Velocity features\n",
    "df_sorted['Velocity'] = 1 / (df_sorted['Time_Delta'] + 1)  # Add 1 to avoid division by zero\n",
    "\n",
    "# Rolling window features (simulated customer grouping)\n",
    "# Note: In real scenarios, you'd group by customer ID\n",
    "window_sizes = [5, 10, 20]\n",
    "for window in window_sizes:\n",
    "    df_sorted[f'Amount_Rolling_Mean_{window}'] = df_sorted['Amount'].rolling(window=window, min_periods=1).mean()\n",
    "    df_sorted[f'Amount_Rolling_Std_{window}'] = df_sorted['Amount'].rolling(window=window, min_periods=1).std()\n",
    "    df_sorted[f'Count_Rolling_{window}'] = df_sorted['Amount'].rolling(window=window, min_periods=1).count()\n",
    "\n",
    "print(\"Velocity features created:\")\n",
    "print(\"- Time_Delta (time between transactions)\")\n",
    "print(\"- Velocity (transaction frequency)\")\n",
    "print(\"- Rolling statistics for windows: 5, 10, 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Creating Statistical Features from V1-V28...\n",
      "------------------------------\n",
      "Statistical features created:\n",
      "- V_Mean, V_Std, V_Min, V_Max, V_Range\n",
      "- V_Skew, V_Kurt (distribution shape)\n",
      "- V_Outliers (count of extreme values)\n"
     ]
    }
   ],
   "source": [
    "# 4. STATISTICAL FEATURES FROM PCA COMPONENTS\n",
    "print(\"\\n4. Creating Statistical Features from V1-V28...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# V1-V28 are PCA components, create additional statistical features\n",
    "v_columns = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "# Statistical aggregations\n",
    "df_sorted['V_Mean'] = df_sorted[v_columns].mean(axis=1)\n",
    "df_sorted['V_Std'] = df_sorted[v_columns].std(axis=1)\n",
    "df_sorted['V_Min'] = df_sorted[v_columns].min(axis=1)\n",
    "df_sorted['V_Max'] = df_sorted[v_columns].max(axis=1)\n",
    "df_sorted['V_Range'] = df_sorted['V_Max'] - df_sorted['V_Min']\n",
    "\n",
    "# Skewness and kurtosis\n",
    "from scipy import stats\n",
    "df_sorted['V_Skew'] = df_sorted[v_columns].apply(lambda x: stats.skew(x), axis=1)\n",
    "df_sorted['V_Kurt'] = df_sorted[v_columns].apply(lambda x: stats.kurtosis(x), axis=1)\n",
    "\n",
    "# Number of outliers (values beyond 2 standard deviations)\n",
    "df_sorted['V_Outliers'] = df_sorted[v_columns].apply(\n",
    "    lambda x: np.sum(np.abs(x) > 2), axis=1\n",
    ")\n",
    "\n",
    "print(\"Statistical features created:\")\n",
    "print(\"- V_Mean, V_Std, V_Min, V_Max, V_Range\")\n",
    "print(\"- V_Skew, V_Kurt (distribution shape)\")\n",
    "print(\"- V_Outliers (count of extreme values)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Creating Interaction Features...\n",
      "------------------------------\n",
      "Interaction features created:\n",
      "- Amount with temporal features\n",
      "- Selected V component interactions\n"
     ]
    }
   ],
   "source": [
    "# 5. INTERACTION FEATURES\n",
    "print(\"\\n5. Creating Interaction Features...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Amount interactions with time\n",
    "df_sorted['Amount_Hour_Interaction'] = df_sorted['Amount'] * df_sorted['Hour']\n",
    "df_sorted['Amount_Weekend_Interaction'] = df_sorted['Amount'] * df_sorted['Is_Weekend']\n",
    "df_sorted['Amount_Night_Interaction'] = df_sorted['Amount'] * df_sorted['Is_Night']\n",
    "\n",
    "# V component interactions (select a few important ones)\n",
    "df_sorted['V1_V2_Interaction'] = df_sorted['V1'] * df_sorted['V2']\n",
    "df_sorted['V1_Amount_Interaction'] = df_sorted['V1'] * df_sorted['Amount_log']\n",
    "df_sorted['V4_V11_Interaction'] = df_sorted['V4'] * df_sorted['V11']\n",
    "\n",
    "print(\"Interaction features created:\")\n",
    "print(\"- Amount with temporal features\")\n",
    "print(\"- Selected V component interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Creating Anomaly Detection Features...\n",
      "------------------------------\n",
      "Anomaly detection features created:\n",
      "- Anomaly_Score (Isolation Forest)\n",
      "- Distance_from_Mean (Euclidean distance)\n"
     ]
    }
   ],
   "source": [
    "# 6. ANOMALY DETECTION FEATURES\n",
    "print(\"\\n6. Creating Anomaly Detection Features...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Isolation Forest score (simplified version)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Use a subset of features for anomaly detection\n",
    "anomaly_features = ['Amount_log', 'V1', 'V2', 'V3', 'V4', 'V5', 'Hour']\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "df_sorted['Anomaly_Score'] = iso_forest.fit_predict(df_sorted[anomaly_features])\n",
    "df_sorted['Anomaly_Score'] = (df_sorted['Anomaly_Score'] == -1).astype(int)\n",
    "\n",
    "# Distance-based features\n",
    "df_sorted['Distance_from_Mean'] = np.sqrt(\n",
    "    ((df_sorted[v_columns] - df_sorted[v_columns].mean()) ** 2).sum(axis=1)\n",
    ")\n",
    "\n",
    "print(\"Anomaly detection features created:\")\n",
    "print(\"- Anomaly_Score (Isolation Forest)\")\n",
    "print(\"- Distance_from_Mean (Euclidean distance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Scaling Features...\n",
      "------------------------------\n",
      "One-hot encoded categorical features: ['Amount_Category']\n",
      "Features scaled using RobustScaler\n",
      "Total features after encoding: 70\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. Scaling Features...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = [col for col in df_sorted.columns if col not in ['Class', 'Time']]\n",
    "X = df_sorted[feature_columns].copy()\n",
    "y = df_sorted['Class'].copy()\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_features = []\n",
    "if 'Amount_Category' in X.columns:\n",
    "    categorical_features = ['Amount_Category']\n",
    "    # One-hot encode categorical features\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "    print(f\"One-hot encoded categorical features: {categorical_features}\")\n",
    "else:\n",
    "    X_encoded = X.copy()\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = RobustScaler()  # Robust to outliers\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_encoded.columns)\n",
    "\n",
    "print(f\"Features scaled using RobustScaler\")\n",
    "print(f\"Total features after encoding: {X_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Feature Selection...\n",
      "------------------------------\n",
      "Missing values before cleaning: 0\n",
      "Infinite values before cleaning: 0\n",
      "Missing values after cleaning: 0\n",
      "Infinite values after cleaning: 0\n",
      "Top 10 most important features:\n",
      "                   feature  importance\n",
      "30                     Day    0.045694\n",
      "35       Is_Business_Hours    0.039235\n",
      "58              V_Outliers    0.016047\n",
      "41                Velocity    0.012371\n",
      "67  Amount_Category_Medium    0.009945\n",
      "16                     V17    0.008258\n",
      "13                     V14    0.008136\n",
      "11                     V12    0.007601\n",
      "9                      V10    0.007530\n",
      "64      V4_V11_Interaction    0.007205\n",
      "Selected 50 most important features\n"
     ]
    }
   ],
   "source": [
    "# 8. FEATURE SELECTION\n",
    "print(\"\\n8. Feature Selection...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Handle missing values before feature selection\n",
    "print(f\"Missing values before cleaning: {X_scaled.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values before cleaning: {np.isinf(X_scaled).sum().sum()}\")\n",
    "\n",
    "# Replace infinite values with NaN, then fill NaN values\n",
    "X_scaled = X_scaled.replace([np.inf, -np.inf], np.nan)\n",
    "X_scaled = X_scaled.fillna(X_scaled.median())\n",
    "\n",
    "print(f\"Missing values after cleaning: {X_scaled.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values after cleaning: {np.isinf(X_scaled).sum().sum()}\")\n",
    "\n",
    "# Calculate feature importance using mutual information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(X_scaled, y, random_state=42)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'importance': mi_scores\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Select top features (limit to available features if less than 50)\n",
    "n_features_to_select = min(50, len(feature_importance))\n",
    "top_features = feature_importance.head(n_features_to_select)['feature'].tolist()\n",
    "X_selected = X_scaled[top_features]\n",
    "\n",
    "print(f\"Selected {len(top_features)} most important features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Creating Train-Test Split...\n",
      "------------------------------\n",
      "Training set shape: (227845, 50)\n",
      "Test set shape: (56962, 50)\n",
      "Training set fraud rate: 0.17%\n",
      "Test set fraud rate: 0.17%\n"
     ]
    }
   ],
   "source": [
    "# 9. TRAIN-TEST SPLIT\n",
    "print(\"\\n9. Creating Train-Test Split...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training set fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test set fraud rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Saving Processed Data...\n",
      "------------------------------\n",
      "Data saved to ../data/processed/:\n",
      "- X_train.csv, X_test.csv\n",
      "- y_train.csv, y_test.csv\n",
      "- scaler.pkl, selected_features.pkl, feature_importance.pkl\n"
     ]
    }
   ],
   "source": [
    "# 10. SAVE PROCESSED DATA\n",
    "print(\"\\n10. Saving Processed Data...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save the processed datasets\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save feature names and scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../data/processed/scaler.pkl')\n",
    "joblib.dump(top_features, '../data/processed/selected_features.pkl')\n",
    "joblib.dump(feature_importance, '../data/processed/feature_importance.pkl')\n",
    "\n",
    "print(\"Data saved to ../data/processed/:\")\n",
    "print(\"- X_train.csv, X_test.csv\")\n",
    "print(\"- y_train.csv, y_test.csv\")\n",
    "print(\"- scaler.pkl, selected_features.pkl, feature_importance.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. Feature Engineering Summary\n",
      "==================================================\n",
      "Original features: 42\n",
      "Engineered features: 50\n",
      "Total samples: 284807\n",
      "Training samples: 227845\n",
      "Test samples: 56962\n",
      "\n",
      "Feature Categories Created:\n",
      "1. Temporal Features (7 features)\n",
      "2. Amount-based Features (6 features)\n",
      "3. Velocity Features (10 features)\n",
      "4. Statistical Features (8 features)\n",
      "5. Interaction Features (6 features)\n",
      "6. Anomaly Detection Features (2 features)\n",
      "7. Original PCA Components (28 features)\n",
      "\n",
      "Data Quality Checks:\n",
      "Missing values: 0\n",
      "Infinite values: 0\n",
      "Feature correlation (max): 1.000\n",
      "\n",
      "Feature engineering completed successfully!\n",
      "Data is ready for model training in 03_model_training.ipynb\n"
     ]
    }
   ],
   "source": [
    "# 11. FEATURE ENGINEERING SUMMARY\n",
    "print(\"\\n11. Feature Engineering Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Original features: {len([col for col in df.columns if col != 'Class'])}\")\n",
    "print(f\"Engineered features: {X_selected.shape[1]}\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "print(\"\\nFeature Categories Created:\")\n",
    "print(\"1. Temporal Features (7 features)\")\n",
    "print(\"2. Amount-based Features (6 features)\")\n",
    "print(\"3. Velocity Features (10 features)\")\n",
    "print(\"4. Statistical Features (8 features)\")\n",
    "print(\"5. Interaction Features (6 features)\")\n",
    "print(\"6. Anomaly Detection Features (2 features)\")\n",
    "print(\"7. Original PCA Components (28 features)\")\n",
    "\n",
    "print(\"\\nData Quality Checks:\")\n",
    "print(f\"Missing values: {X_selected.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values: {np.isinf(X_selected).sum().sum()}\")\n",
    "print(f\"Feature correlation (max): {X_selected.corr().abs().max().max():.3f}\")\n",
    "\n",
    "print(\"\\nFeature engineering completed successfully!\")\n",
    "print(\"Data is ready for model training in 03_model_training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
